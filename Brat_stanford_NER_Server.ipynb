{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file pair: 899872.ann and 899872.txt\n",
      "Processed file pair: 899874.ann and 899874.txt\n",
      "Processed file pair: 899876.ann and 899876.txt\n",
      "Processed file pair: 899878.ann and 899878.txt\n",
      "Processed file pair: 899880.ann and 899880.txt\n",
      "Processed file pair: 899882.ann and 899882.txt\n",
      "Processed file pair: 899884.ann and 899884.txt\n",
      "Processed file pair: 899888.ann and 899888.txt\n",
      "Processed file pair: 899890.ann and 899890.txt\n",
      "Processed file pair: 899892.ann and 899892.txt\n",
      "Processed file pair: 899894.ann and 899894.txt\n",
      "Processed file pair: 899896.ann and 899896.txt\n",
      "Processed file pair: 899898.ann and 899898.txt\n",
      "Processed file pair: 899900.ann and 899900.txt\n",
      "Processed file pair: 899902.ann and 899902.txt\n",
      "Processed file pair: 899904.ann and 899904.txt\n",
      "Processed file pair: 899906.ann and 899906.txt\n",
      "Processed file pair: 899908.ann and 899908.txt\n",
      "Processed file pair: 899910.ann and 899910.txt\n",
      "Processed file pair: 899912.ann and 899912.txt\n",
      "Processed file pair: 899914.ann and 899914.txt\n",
      "Processed file pair: 899916.ann and 899916.txt\n",
      "Processed file pair: 899918.ann and 899918.txt\n",
      "Processed file pair: 899920.ann and 899920.txt\n",
      "Processed file pair: 899922.ann and 899922.txt\n",
      "Processed file pair: 899926.ann and 899926.txt\n",
      "Processed file pair: 899932.ann and 899932.txt\n",
      "Processed file pair: 899934.ann and 899934.txt\n",
      "Processed file pair: 899936.ann and 899936.txt\n",
      "Processed file pair: 899938.ann and 899938.txt\n",
      "Processed file pair: 899940.ann and 899940.txt\n",
      "Processed file pair: 899944.ann and 899944.txt\n",
      "Processed file pair: 899946.ann and 899946.txt\n",
      "Processed file pair: 899948.ann and 899948.txt\n",
      "Processed file pair: 899950.ann and 899950.txt\n",
      "Processed file pair: 899952.ann and 899952.txt\n",
      "Processed file pair: 899954.ann and 899954.txt\n",
      "Processed file pair: 899956.ann and 899956.txt\n",
      "Processed file pair: 899958.ann and 899958.txt\n",
      "Processed file pair: 899960.ann and 899960.txt\n",
      "Processed file pair: 899962.ann and 899962.txt\n",
      "Processed file pair: 899964.ann and 899964.txt\n",
      "Processed file pair: 899968.ann and 899968.txt\n",
      "Processed file pair: 899970.ann and 899970.txt\n",
      "Processed file pair: 899972.ann and 899972.txt\n",
      "Processed file pair: 899974.ann and 899974.txt\n",
      "Processed file pair: 899976.ann and 899976.txt\n",
      "Processed file pair: 899978.ann and 899978.txt\n",
      "Processed file pair: 899980.ann and 899980.txt\n",
      "Processed file pair: 899982.ann and 899982.txt\n",
      "Processed file pair: 899984.ann and 899984.txt\n",
      "Processed file pair: 899986.ann and 899986.txt\n",
      "Processed file pair: 899988.ann and 899988.txt\n",
      "Processed file pair: 899990.ann and 899990.txt\n",
      "Processed file pair: 899992.ann and 899992.txt\n",
      "Processed file pair: 899994.ann and 899994.txt\n",
      "Processed file pair: 899996.ann and 899996.txt\n",
      "Processed file pair: 899998.ann and 899998.txt\n",
      "Processed file pair: 900002.ann and 900002.txt\n",
      "Processed file pair: 900004.ann and 900004.txt\n",
      "Processed file pair: 900006.ann and 900006.txt\n",
      "Processed file pair: 900010.ann and 900010.txt\n",
      "Processed file pair: 900012.ann and 900012.txt\n",
      "Processed file pair: 900014.ann and 900014.txt\n",
      "Processed file pair: 900016.ann and 900016.txt\n",
      "Processed file pair: 900020.ann and 900020.txt\n",
      "Processed file pair: 900024.ann and 900024.txt\n",
      "Processed file pair: 900026.ann and 900026.txt\n",
      "Processed file pair: 900030.ann and 900030.txt\n",
      "Processed file pair: 900032.ann and 900032.txt\n",
      "Processed file pair: 900034.ann and 900034.txt\n",
      "Processed file pair: 900036.ann and 900036.txt\n",
      "Processed file pair: 900038.ann and 900038.txt\n",
      "Processed file pair: 900040.ann and 900040.txt\n",
      "Processed file pair: 900042.ann and 900042.txt\n",
      "Processed file pair: 900044.ann and 900044.txt\n",
      "Processed file pair: 900046.ann and 900046.txt\n",
      "Processed file pair: 900048.ann and 900048.txt\n",
      "Processed file pair: 900050.ann and 900050.txt\n",
      "Processed file pair: 900052.ann and 900052.txt\n",
      "Processed file pair: 900054.ann and 900054.txt\n",
      "Processed file pair: 900056.ann and 900056.txt\n",
      "Processed file pair: 900058.ann and 900058.txt\n",
      "Processed file pair: 900060.ann and 900060.txt\n",
      "Processed file pair: 900062.ann and 900062.txt\n",
      "Processed file pair: 900064.ann and 900064.txt\n",
      "Processed file pair: 900066.ann and 900066.txt\n",
      "Processed file pair: 900068.ann and 900068.txt\n",
      "Processed file pair: 900070.ann and 900070.txt\n",
      "Processed file pair: 900072.ann and 900072.txt\n",
      "Processed file pair: 900074.ann and 900074.txt\n",
      "Processed file pair: 900076.ann and 900076.txt\n",
      "Processed file pair: 900078.ann and 900078.txt\n",
      "Processed file pair: 900080.ann and 900080.txt\n",
      "Processed file pair: 900084.ann and 900084.txt\n",
      "Processed file pair: 900086.ann and 900086.txt\n",
      "Processed file pair: 900090.ann and 900090.txt\n",
      "Processed file pair: 900092.ann and 900092.txt\n",
      "Processed file pair: 900094.ann and 900094.txt\n",
      "Processed file pair: 900096.ann and 900096.txt\n",
      "Processed file pair: 900100.ann and 900100.txt\n",
      "Processed file pair: 900102.ann and 900102.txt\n",
      "Processed file pair: 900104.ann and 900104.txt\n",
      "Processed file pair: 900108.ann and 900108.txt\n",
      "Processed file pair: 900110.ann and 900110.txt\n",
      "Processed file pair: 900112.ann and 900112.txt\n",
      "Processed file pair: 900114.ann and 900114.txt\n",
      "Processed file pair: 900116.ann and 900116.txt\n",
      "Processed file pair: 900120.ann and 900120.txt\n",
      "Processed file pair: 900122.ann and 900122.txt\n",
      "Processed file pair: 900124.ann and 900124.txt\n",
      "Processed file pair: 900126.ann and 900126.txt\n",
      "Processed file pair: 900128.ann and 900128.txt\n",
      "Processed file pair: 900130.ann and 900130.txt\n",
      "Processed file pair: 900132.ann and 900132.txt\n",
      "Processed file pair: 900134.ann and 900134.txt\n",
      "Processed file pair: 900136.ann and 900136.txt\n",
      "Processed file pair: 900138.ann and 900138.txt\n",
      "Processed file pair: 900140.ann and 900140.txt\n",
      "Processed file pair: 900142.ann and 900142.txt\n",
      "Processed file pair: 900144.ann and 900144.txt\n",
      "Processed file pair: 900146.ann and 900146.txt\n",
      "Processed file pair: 900148.ann and 900148.txt\n",
      "Processed file pair: 900150.ann and 900150.txt\n",
      "Processed file pair: 900156.ann and 900156.txt\n",
      "Processed file pair: 900158.ann and 900158.txt\n",
      "Processed file pair: 900160.ann and 900160.txt\n",
      "Processed file pair: 900168.ann and 900168.txt\n",
      "Processed file pair: 900170.ann and 900170.txt\n",
      "Processed file pair: 900174.ann and 900174.txt\n",
      "Processed file pair: 900176.ann and 900176.txt\n",
      "Processed file pair: 900178.ann and 900178.txt\n",
      "Processed file pair: 900180.ann and 900180.txt\n",
      "Processed file pair: 900182.ann and 900182.txt\n",
      "Processed file pair: 900184.ann and 900184.txt\n",
      "Processed file pair: 900186.ann and 900186.txt\n",
      "Processed file pair: 900188.ann and 900188.txt\n",
      "Processed file pair: 900190.ann and 900190.txt\n",
      "Processed file pair: 900192.ann and 900192.txt\n",
      "Processed file pair: 900194.ann and 900194.txt\n",
      "Processed file pair: 900196.ann and 900196.txt\n",
      "Processed file pair: 900198.ann and 900198.txt\n",
      "Processed file pair: 900202.ann and 900202.txt\n",
      "Processed file pair: 900204.ann and 900204.txt\n",
      "Processed file pair: 900206.ann and 900206.txt\n",
      "Processed file pair: 900208.ann and 900208.txt\n",
      "Processed file pair: 900210.ann and 900210.txt\n",
      "Processed file pair: 900212.ann and 900212.txt\n",
      "Processed file pair: 900214.ann and 900214.txt\n",
      "Processed file pair: 900216.ann and 900216.txt\n",
      "Processed file pair: 900218.ann and 900218.txt\n",
      "Processed file pair: 900220.ann and 900220.txt\n",
      "Processed file pair: 900222.ann and 900222.txt\n",
      "Processed file pair: 900224.ann and 900224.txt\n",
      "Processed file pair: 900226.ann and 900226.txt\n",
      "Processed file pair: 900228.ann and 900228.txt\n",
      "Processed file pair: 900230.ann and 900230.txt\n",
      "Processed file pair: 900232.ann and 900232.txt\n",
      "Processed file pair: 900234.ann and 900234.txt\n",
      "Processed file pair: 900238.ann and 900238.txt\n",
      "Processed file pair: 900240.ann and 900240.txt\n",
      "Processed file pair: 900244.ann and 900244.txt\n",
      "Processed file pair: 900246.ann and 900246.txt\n",
      "Processed file pair: 900248.ann and 900248.txt\n",
      "Processed file pair: 900250.ann and 900250.txt\n",
      "Processed file pair: 900252.ann and 900252.txt\n",
      "Processed file pair: 900254.ann and 900254.txt\n",
      "Processed file pair: 900256.ann and 900256.txt\n",
      "Processed file pair: 900258.ann and 900258.txt\n",
      "Processed file pair: 900260.ann and 900260.txt\n",
      "Processed file pair: 900262.ann and 900262.txt\n",
      "Processed file pair: 900264.ann and 900264.txt\n",
      "Processed file pair: 900266.ann and 900266.txt\n",
      "Processed file pair: 900270.ann and 900270.txt\n",
      "Processed file pair: 900272.ann and 900272.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file pair: 900274.ann and 900274.txt\n",
      "Processed file pair: 900276.ann and 900276.txt\n",
      "Processed file pair: 900278.ann and 900278.txt\n",
      "Processed file pair: 900282.ann and 900282.txt\n",
      "Processed file pair: 900284.ann and 900284.txt\n",
      "Processed file pair: 900286.ann and 900286.txt\n",
      "Processed file pair: 900288.ann and 900288.txt\n",
      "Processed file pair: 900290.ann and 900290.txt\n",
      "Processed file pair: 900292.ann and 900292.txt\n",
      "Processed file pair: 900298.ann and 900298.txt\n",
      "Processed file pair: 900302.ann and 900302.txt\n",
      "Processed file pair: 900312.ann and 900312.txt\n",
      "Processed file pair: 900320.ann and 900320.txt\n",
      "Processed file pair: 900324.ann and 900324.txt\n",
      "Processed file pair: 900328.ann and 900328.txt\n",
      "Processed file pair: 900334.ann and 900334.txt\n",
      "Processed file pair: 900342.ann and 900342.txt\n",
      "Processed file pair: 900344.ann and 900344.txt\n",
      "Processed file pair: 900346.ann and 900346.txt\n",
      "Processed file pair: 900348.ann and 900348.txt\n",
      "Processed file pair: 900350.ann and 900350.txt\n",
      "Processed file pair: 900352.ann and 900352.txt\n",
      "Processed file pair: 900354.ann and 900354.txt\n",
      "Processed file pair: 900356.ann and 900356.txt\n"
     ]
    }
   ],
   "source": [
    "# A python script to turn annotated data in standoff format (brat annotation tool) to the formats expected by Stanford NER and Relation Extractor models\n",
    "# - NER format based on: http://nlp.stanford.edu/software/crf-faq.html#a\n",
    "# - RE format based on: http://nlp.stanford.edu/software/relationExtractor.html#training\n",
    "\n",
    "# Usage:\n",
    "# 1) Install the pycorenlp package\n",
    "# 2) Run CoreNLP server (change CORENLP_SERVER_ADDRESS if needed)\n",
    "# 3) Place .ann and .txt files from brat in the location specified in DATA_DIRECTORY\n",
    "# 4) Run this script\n",
    "\n",
    "# Cross-sentence annotation is not supported\n",
    "\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "DEFAULT_OTHER_ANNO = 'O'\n",
    "STANDOFF_ENTITY_PREFIX = 'T'\n",
    "STANDOFF_RELATION_PREFIX = 'R'\n",
    "DATA_DIRECTORY = \"C:\\\\Users\\\\rohin\\\\Rohini_Mondaq_Dessertation\\\\stanford_ner\\\\testing\"\n",
    "OUTPUT_DIRECTORY = 'C:\\\\Users\\\\rohin\\\\Rohini_Mondaq_Dessertation\\\\stanford_ner\\\\testing_results'\n",
    "CORENLP_SERVER_ADDRESS = 'http://localhost:9000'\n",
    "\n",
    "NER_TRAINING_DATA_OUTPUT_PATH = join(OUTPUT_DIRECTORY, 'ner-crf-testing-data.tsv')\n",
    "RE_TRAINING_DATA_OUTPUT_PATH = join(OUTPUT_DIRECTORY, 're-testing-data.corp')\n",
    "\n",
    "if os.path.exists(OUTPUT_DIRECTORY):\n",
    "\tif os.path.exists(NER_TRAINING_DATA_OUTPUT_PATH):\n",
    "\t\tos.remove(NER_TRAINING_DATA_OUTPUT_PATH)\n",
    "\tif os.path.exists(RE_TRAINING_DATA_OUTPUT_PATH):\n",
    "\t\tos.remove(RE_TRAINING_DATA_OUTPUT_PATH)\n",
    "else:\n",
    "    os.makedirs(OUTPUT_DIRECTORY)\n",
    "\n",
    "sentence_count = 0\n",
    "nlp = StanfordCoreNLP(CORENLP_SERVER_ADDRESS)\n",
    "\n",
    "# looping through .ann files in the data directory\n",
    "ann_data_files = [f for f in listdir(DATA_DIRECTORY) if isfile(join(DATA_DIRECTORY, f)) and f.split('.')[1] == 'ann']\n",
    "\n",
    "for file in ann_data_files:\n",
    "\tentities = []\n",
    "\trelations = []\n",
    "\n",
    "\t# process .ann file - place entities and relations into 2 seperate lists of tuples\n",
    "\twith open(join(DATA_DIRECTORY, file), 'r',encoding=\"utf8\") as document_anno_file:\n",
    "\t\tlines = document_anno_file.readlines()\n",
    "\t\tfor line in lines:\n",
    "\t\t\tstandoff_line = line.split()\n",
    "\t\t\tif standoff_line[0][0] == STANDOFF_ENTITY_PREFIX:\n",
    "\t\t\t\tentity = {}\n",
    "\t\t\t\tentity['standoff_id'] = int(standoff_line[0][1:])\n",
    "\t\t\t\tentity['entity_type'] = standoff_line[1].capitalize()\n",
    "\t\t\t\tentity['offset_start'] = int(standoff_line[2])\n",
    "\t\t\t\tentity['offset_end'] = int(standoff_line[3])\n",
    "\t\t\t\tentity['word'] = standoff_line[4]\n",
    "\t\t\t\tentities.append(entity)\n",
    "\n",
    "\t\t\telif standoff_line[0][0] == STANDOFF_RELATION_PREFIX:\n",
    "\t\t\t\trelation = {}\n",
    "\t\t\t\trelation['standoff_id'] = int(standoff_line[0][1:])\n",
    "\t\t\t\trelation['name'] = standoff_line[1]\n",
    "\t\t\t\trelation['standoff_entity1_id'] = int(standoff_line[2].split(':')[1][1:])\n",
    "\t\t\t\trelation['standoff_entity2_id'] = int(standoff_line[3].split(':')[1][1:])\n",
    "\t\t\t\trelations.append(relation)\n",
    "\t\t\t\t# relations.append((standoff_id, relation_name, standoff_entity1_id, standoff_entity2_id))\n",
    "\n",
    "\t# read the .ann's matching .txt file and tokenize its text using stanford corenlp\n",
    "\twith open(join(DATA_DIRECTORY, file.replace('.ann', '.txt')), 'r',encoding=\"utf8\") as document_text_file:\n",
    "\t\tdocument_text = document_text_file.read()\n",
    "\n",
    "\toutput = nlp.annotate(document_text, properties={\n",
    "\t  'annotators': 'tokenize,ssplit,pos',\n",
    "\t  'outputFormat': 'json'\n",
    "\t})\n",
    "\n",
    "\t# write text and annotations into NER and RE output files\n",
    "\twith open(NER_TRAINING_DATA_OUTPUT_PATH, 'a',encoding=\"utf-8\") as ner_training_data, open(RE_TRAINING_DATA_OUTPUT_PATH, 'a',encoding=\"utf-8\") as re_training_data:\n",
    "\t\tfor sentence in output['sentences']:\n",
    "\t\t\tentities_in_sentence = {}\n",
    "\t\t\tsentence_re_rows = []\n",
    "\n",
    "\t\t\tfor token in sentence['tokens']:\n",
    "\t\t\t\toffset_start = int(token['characterOffsetBegin'])\n",
    "\t\t\t\toffset_end = int(token['characterOffsetEnd'])\n",
    "\n",
    "\t\t\t\tre_row = {}\n",
    "\t\t\t\tentity_found = False\n",
    "\t\t\t\tner_anno = DEFAULT_OTHER_ANNO\n",
    "\n",
    "\t\t\t\t# searching for token in annotated entities\n",
    "\t\t\t\tfor entity in entities:\n",
    "\t\t\t\t\tif offset_start >= entity['offset_start'] and offset_end <= entity['offset_end']:\n",
    "\t\t\t\t\t\tner_anno = entity['entity_type']\n",
    "\n",
    "\t\t\t\t\t# multi-token entities for RE need to be handled differently than NER\n",
    "\t\t\t\t\tif offset_start == entity['offset_start'] and offset_end <= entity['offset_end']:\n",
    "\t\t\t\t\t\tentities_in_sentence[entity['standoff_id']] = len(sentence_re_rows)\n",
    "\t\t\t\t\t\tre_row['entity_type'] = entity['entity_type']\n",
    "\t\t\t\t\t\tre_row['pos_tag'] = token['pos']\n",
    "\t\t\t\t\t\tre_row['word'] = token['word']\n",
    "\n",
    "\t\t\t\t\t\tsentence_re_rows.append(re_row)\n",
    "\t\t\t\t\t\tentity_found = True\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\telif offset_start > entity['offset_start'] and offset_end <= entity['offset_end'] and len(sentence_re_rows) > 0:\n",
    "\t\t\t\t\t\tsentence_re_rows[-1]['pos_tag'] += '/{}'.format(token['pos'])\n",
    "\t\t\t\t\t\tsentence_re_rows[-1]['word'] += '/{}'.format(token['word'])\n",
    "\t\t\t\t\t\tentity_found = True\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\n",
    "\t\t\t\tif not entity_found:\n",
    "\t\t\t\t\tre_row['entity_type'] = DEFAULT_OTHER_ANNO\n",
    "\t\t\t\t\tre_row['pos_tag'] = token['pos']\n",
    "\t\t\t\t\tre_row['word'] = token['word']\n",
    "\n",
    "\t\t\t\t\tsentence_re_rows.append(re_row)\n",
    "\n",
    "\t\t\t\t# writing tagged tokens to NER training data\n",
    "\t\t\t\tner_training_data.write('{}\\t{}\\n'.format(token['word'], ner_anno))\n",
    "\n",
    "\t\t\t# writing tagged tokens to RE training data\n",
    "\t\t\ttoken_count = 0\n",
    "\t\t\tfor sentence_row in sentence_re_rows:\n",
    "\t\t\t\tre_training_data.write('{}\\t{}\\t{}\\tO\\t{}\\t{}\\tO\\tO\\tO\\n'.format(str(sentence_count), sentence_row['entity_type'], str(token_count), sentence_row['pos_tag'], sentence_row['word']))\n",
    "\t\t\t\ttoken_count += 1\n",
    "\n",
    "\t\t\tre_training_data.write('\\n')\n",
    "\n",
    "\t\t\t# writing relations to RE training data\n",
    "\t\t\tfor relation in relations:\n",
    "\t\t\t\tif relation['standoff_entity1_id'] in entities_in_sentence and relation['standoff_entity2_id'] in entities_in_sentence:\n",
    "\t\t\t\t\tentity1 = str(entities_in_sentence[relation['standoff_entity1_id']])\n",
    "\t\t\t\t\tentity2 = str(entities_in_sentence[relation['standoff_entity2_id']])\n",
    "\t\t\t\t\trelation_name = relation['name']\n",
    "\t\t\t\t\tre_training_data.write('{}\\t{}\\t{}\\n'.format(entity1, entity2, relation_name))\n",
    "\n",
    "\t\t\tre_training_data.write('\\n')\n",
    "\n",
    "\t\t\tsentence_count += 1\n",
    "\n",
    "\t\tner_training_data.write('\\n')\n",
    "\n",
    "\tprint('Processed file pair: {} and {}'.format(file, file.replace('.ann', '.txt')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'sentencebreaks_to_newlines' from 'sentencesplit' (C:\\Users\\rohin\\sentencesplit.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-dd0e2c566dec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mos\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msentencesplit\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msentencebreaks_to_newlines\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# assume script in brat tools/ directory, extend path to find sentencesplit.py\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'sentencebreaks_to_newlines' from 'sentencesplit' (C:\\Users\\rohin\\sentencesplit.py)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
