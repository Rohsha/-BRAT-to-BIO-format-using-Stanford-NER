{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A python script to turn annotated data in standoff format (brat annotation tool) to the formats expected by Stanford NER and Relation Extractor models\n",
    "# - NER format based on: http://nlp.stanford.edu/software/crf-faq.html#a\n",
    "# - RE format based on: http://nlp.stanford.edu/software/relationExtractor.html#training\n",
    "\n",
    "# Usage:\n",
    "# 1) Install the pycorenlp package\n",
    "# 2) Run CoreNLP server (change CORENLP_SERVER_ADDRESS if needed)\n",
    "# 3) Place .ann and .txt files from brat in the location specified in DATA_DIRECTORY\n",
    "# 4) Run this script\n",
    "\n",
    "# Cross-sentence annotation is not supported\n",
    "\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "DEFAULT_OTHER_ANNO = 'O'\n",
    "STANDOFF_ENTITY_PREFIX = 'T'\n",
    "DATA_DIRECTORY = \"C:\\\\Users\\\\rohin\\\\Rohini_Mondaq_Dessertation\\\\mondaq_data_project1_set2\"\n",
    "OUTPUT_DIRECTORY = 'new_output'\n",
    "CORENLP_SERVER_ADDRESS = 'http://localhost:9000'\n",
    "\n",
    "NER_TRAINING_DATA_OUTPUT_PATH = join(OUTPUT_DIRECTORY, 'ner-crf-training-data1.tsv')\n",
    "RE_TRAINING_DATA_OUTPUT_PATH = join(OUTPUT_DIRECTORY, 're-training-data1.corp')\n",
    "\n",
    "if os.path.exists(OUTPUT_DIRECTORY):\n",
    "\tif os.path.exists(NER_TRAINING_DATA_OUTPUT_PATH):\n",
    "\t\tos.remove(NER_TRAINING_DATA_OUTPUT_PATH)\n",
    "\tif os.path.exists(RE_TRAINING_DATA_OUTPUT_PATH):\n",
    "\t\tos.remove(RE_TRAINING_DATA_OUTPUT_PATH)\n",
    "else:\n",
    "    os.makedirs(OUTPUT_DIRECTORY)\n",
    "\n",
    "sentence_count = 1\n",
    "nlp = StanfordCoreNLP(CORENLP_SERVER_ADDRESS)\n",
    "\n",
    "# looping through .ann files in the data directory\n",
    "ann_data_files = [f for f in listdir(DATA_DIRECTORY) if isfile(join(DATA_DIRECTORY, f)) and f.split('.')[1] == 'ann']\n",
    "\n",
    "for file in ann_data_files:\n",
    "\tentities = []\n",
    "\n",
    "\t# process .ann file - place entities and relations into 2 seperate lists of tuples\n",
    "\twith open(join(DATA_DIRECTORY, file), 'r',encoding=\"utf8\") as document_anno_file:\n",
    "\t\tlines = document_anno_file.readlines()\n",
    "\t\tfor line in lines:\n",
    "\t\t\tstandoff_line = line.split()\n",
    "\t\t\tif standoff_line[0][0] == STANDOFF_ENTITY_PREFIX:\n",
    "\t\t\t\tentity = {}\n",
    "\t\t\t\tentity['standoff_id'] = int(standoff_line[0][1:])\n",
    "\t\t\t\tentity['entity_type'] = standoff_line[1].capitalize()\n",
    "\t\t\t\tentity['offset_start'] = int(standoff_line[2])\n",
    "\t\t\t\tentity['offset_end'] = int(standoff_line[3])\n",
    "\t\t\t\tentity['word'] = standoff_line[4]\n",
    "\t\t\t\tentities.append(entity)\n",
    "\t\tprint(entities)               \n",
    "\n",
    "\n",
    "\t# read the .ann's matching .txt file and tokenize its text using stanford corenlp\n",
    "\t \n",
    "\n",
    "\toutput = nlp.annotate(document_text, properties={\n",
    "\t  'annotators': 'tokenize,ssplit,pos',\n",
    "\t  'outputFormat': 'json'\n",
    "\t})\n",
    "\tprint('output')\n",
    "\tprint(output)    \n",
    "\t# write text and annotations into NER and RE output files\n",
    "\twith open(NER_TRAINING_DATA_OUTPUT_PATH, 'a',encoding=\"utf-8\") as ner_training_data, open(RE_TRAINING_DATA_OUTPUT_PATH, 'a',encoding=\"utf-8\") as re_training_data:\n",
    "\t\tfor sentence in output['sentences']:\n",
    "\t\t\tentities_in_sentence = {}\n",
    "\t\t\tsentence_re_rows = []\n",
    "\n",
    "\t\t\tfor token in sentence['tokens']:\n",
    "\t\t\t\toffset_start = int(token['characterOffsetBegin'])\n",
    "\t\t\t\toffset_end = int(token['characterOffsetEnd'])\n",
    "\n",
    "\t\t\t\tre_row = {}\n",
    "\t\t\t\tentity_found = False\n",
    "\t\t\t\tner_anno = DEFAULT_OTHER_ANNO\n",
    "\n",
    "\t\t\t\t# searching for token in annotated entities\n",
    "\t\t\t\tfor entity in entities:\n",
    "\n",
    "\t\t\t\t\tif offset_start == entity['offset_start'] and offset_end <= entity['offset_end']:\n",
    "\t\t\t\t\t\tner_anno = 'B-'+entity['entity_type']\n",
    "\t\t\t\t\t\tprint ('ner_anno')\n",
    "\t\t\t\t\t\tprint (ner_anno)\n",
    "\n",
    "\t\t\t\t\tif offset_start > entity['offset_start'] and offset_end <= entity['offset_end']:\n",
    "\t\t\t\t\t\tner_anno = 'I-'+entity['entity_type']\n",
    "\t\t\t\t\t\tprint ('ner_anno')\n",
    "\t\t\t\t\t\tprint (ner_anno)\n",
    "\t\t\t\t\t# multi-token entities for RE need to be handled differently than NER\n",
    "\t\t\t\t\tif offset_start == entity['offset_start'] and offset_end <= entity['offset_end']:\n",
    "\t\t\t\t\t\tentities_in_sentence[entity['standoff_id']] = len(sentence_re_rows)\n",
    "\t\t\t\t\t\tre_row['entity_type'] = 'B-'+entity['entity_type']\n",
    "\t\t\t\t\t\tre_row['pos_tag'] = token['pos']\n",
    "\t\t\t\t\t\tre_row['word'] = token['word']\n",
    "\t\t\t\t\t\tprint ('re_row')\n",
    "\t\t\t\t\t\tprint (re_row)\n",
    "\t\t\t\t\t\tsentence_re_rows.append(re_row)\n",
    "\t\t\t\t\t\tentity_found = True\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t#elif offset_start > entity['offset_start'] and offset_end <= entity['offset_end'] and len(sentence_re_rows) > 0:\n",
    "\t\t\t\t\telif offset_start > entity['offset_start'] and offset_end <= entity['offset_end']:                    \n",
    "\t\t\t\t\t\tre_row['entity_type'] = 'I-'+entity['entity_type']\n",
    "\t\t\t\t\t\tre_row['pos_tag'] = token['pos']\n",
    "\t\t\t\t\t\tre_row['word'] = token['word']\n",
    "\t\t\t\t\t\tprint ('re_row')\n",
    "\t\t\t\t\t\tprint (re_row)\n",
    "\t\t\t\t\t\tsentence_re_rows.append(re_row)\n",
    "\t\t\t\t\t\tentity_found = True\n",
    "\t\t\t\t\t\t#sentence_re_rows[-1]['pos_tag'] += '/{}'.format(token['pos'])\n",
    "\t\t\t\t\t\t#sentence_re_rows[-1]['word'] += '/{}'.format(token['word'])\n",
    "\t\t\t\t\t\t#entity_found = True\n",
    "\t\t\t\t\t\t#print ('re_row')\n",
    "\t\t\t\t\t\t#print (re_row)\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\tif not entity_found:\n",
    "\t\t\t\t\tre_row['entity_type'] = DEFAULT_OTHER_ANNO\n",
    "\t\t\t\t\tre_row['pos_tag'] = token['pos']\n",
    "\t\t\t\t\tre_row['word'] = token['word']\n",
    "\t\t\t\t\tsentence_re_rows.append(re_row)\n",
    "\n",
    "\t\t\t\t# writing tagged tokens to NER training data\n",
    "\t\t\t\tner_training_data.write('{}\\t{}\\n'.format(token['word'], ner_anno))\n",
    "\n",
    "\t\t\t# writing tagged tokens to RE training data\n",
    "\t\t\ttoken_count = 0\n",
    "\t\t\tfor sentence_row in sentence_re_rows:\n",
    "\t\t\t\tre_training_data.write('{}\\t{}\\t\\t{}\\t{}\\t{}\\n'.format(str(sentence_count), sentence_row['entity_type'], str(token_count), sentence_row['pos_tag'], sentence_row['word']))\n",
    "\t\t\t\ttoken_count += 1\n",
    "\n",
    "\t\t\tre_training_data.write('\\n')\n",
    "\n",
    "\t\t\t# writing relations to RE training data\n",
    "\t\t\tfor relation in relations:\n",
    "\t\t\t\tif relation['standoff_entity1_id'] in entities_in_sentence and relation['standoff_entity2_id'] in entities_in_sentence:\n",
    "\t\t\t\t\tentity1 = str(entities_in_sentence[relation['standoff_entity1_id']])\n",
    "\t\t\t\t\tentity2 = str(entities_in_sentence[relation['standoff_entity2_id']])\n",
    "\t\t\t\t\trelation_name = relation['name']\n",
    "\t\t\t\t\tre_training_data.write('{}\\t{}\\t{}\\n'.format(entity1, entity2, relation_name))\n",
    "\n",
    "\t\t\tre_training_data.write('\\n')\n",
    "\n",
    "\t\t\tsentence_count += 1\n",
    "\n",
    "\t\tner_training_data.write('\\n')\n",
    "\n",
    "        \n",
    "\tprint('Processed file pair: {} and {}'.format(file, file.replace('.ann', '.txt')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
